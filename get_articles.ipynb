{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install featuretools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/dgunning/cord19.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import subprocess\n",
    "import shutil\n",
    "import hashlib\n",
    "#import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/dask/dataframe/utils.py:14: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from cord import ResearchPapers\n",
    "import featuretools as ft\n",
    "#from featuretools.nlp_primitives import UniversalSentenceEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path information\n",
    "task='task1'# or 'task2' # specify task\n",
    "#root_path='/repo1/code/autoreview/'\n",
    "root_path='./'\n",
    "data_path=root_path+'data/'+task+'/'\n",
    "literature_path=data_path+'CORD-19-research-challenge/'#path to save retrieved articles abstract\n",
    "sentence_file_name='hypercoagulable_sentences.tsv'\n",
    "#sentence_embedding_file_name='hypercoagulable_sentence_USEfeatures.pkl'\n",
    "keywords_of_interest =['anticoagulants','venous thromboembolism', 'thrombotic complications', 'hypercoagulability','clot formation', 'Thrombosis', 'Thrombotic', 'D-Dimer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_literature():\n",
    "    \"\"\"\n",
    "    Download literatures from CORD19, Google Scholar, and Bioarix.\n",
    "    \"\"\"\n",
    "    #Download\n",
    "    #Biorxiv\n",
    "    subprocess.call(['curl -o collection.json https://connect.biorxiv.org/relate/collection_json.php?grp=181'], shell=True)\n",
    "    #CORD19\n",
    "    subprocess.call(['wget https://www.dropbox.com/s/osa58hx8rs5yl3t/metadata.csv?raw=1'],shell=True)\n",
    "    #Google Scholar\n",
    "    subprocess.call(['wget https://www.dropbox.com/s/5incr3c86sh43gq/hypercoagulable_fulltext.xlsx?raw=1'],shell=True)\n",
    "\n",
    "    #Create directory to put literature files\n",
    "    if not os.path.exists(literature_path):\n",
    "        os.makedirs(literature_path)\n",
    "    \n",
    "    #move files to data_path\n",
    "    shutil.move('./metadata.csv?raw=1',literature_path+'metadata_old.csv')\n",
    "    shutil.move('./hypercoagulable_fulltext.xlsx?raw=1',literature_path+'hypercoagulable_fulltext.xlsx')\n",
    "    shutil.move('./collection.json', literature_path+'biorxiv.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge2cord19_metadata():\n",
    "    \"\"\"\n",
    "    Merge the three different dataframes into one dataframe following CORD19 structure\n",
    "    \"\"\"\n",
    "    \n",
    "    def hash(sourcedf,destinationdf,*column):\n",
    "        columnName = ''\n",
    "        destinationdf['sha'] = pd.DataFrame(sourcedf[list(column)].values.sum(axis=1))[0].str.encode('utf-8').apply(lambda x: (hashlib.sha512(x).hexdigest().upper()))\n",
    "        destinationdf['cord_uid'] = destinationdf['sha'].apply(lambda x: x[0:8] )\n",
    "    # hash(df,df,'ID','Salt')\n",
    "\n",
    "    #Load the literature files as pd.Dataframe\n",
    "    df_cord19_meta = pd.read_csv(literature_path+'metadata_old.csv')\n",
    "    df_pap_gscholar = pd.read_excel(literature_path+'hypercoagulable_fulltext.xlsx')\n",
    "    df_biomedRxiv = pd.json_normalize(pd.read_json(literature_path+'biorxiv.json')['rels'],'rel_authors',['rel_title', 'rel_doi',\\\n",
    "                                                                        'rel_link', 'rel_abs', \\\n",
    "                                                                        'rel_date', 'rel_site'])\n",
    "\n",
    "    df_biomedRxiv = df_biomedRxiv.groupby(['rel_title', 'rel_doi', 'rel_link',\n",
    "           'rel_abs', 'rel_date', 'rel_site'])['author_name'].agg(author_name=lambda x: ','.join(x)).reset_index()\n",
    "\n",
    "\n",
    "    # Merge the three different dataframes into `df_cord19_meta_augmented`\n",
    "    dic_pap2cord10 = {'Authors':'authors', 'Title': 'title', \n",
    "                      'Year':'publish_time',  'ArticleURL':'url', 'Source':'journal',\n",
    "                       'DOI':'doi', 'Abstract':'abstract'}\n",
    "    dic_biomedRxiv2cord10 = {\n",
    "        'rel_title':'title', 'rel_doi':'doi' ,\n",
    "        'rel_link':'url', 'rel_abs':'abstract', \n",
    "        'author_name':'authors', 'rel_date':'publish_time', \n",
    "        'rel_site':'journal'\n",
    "    }\n",
    "\n",
    "    #display(df_cord19_meta.head(1))\n",
    "\n",
    "    df_pap_scholar_compatible = df_pap_gscholar[list(dic_pap2cord10.keys())]\n",
    "    df_pap_scholar_compatible.columns = list(dic_pap2cord10.values())\n",
    "    hash(df_pap_scholar_compatible,df_pap_scholar_compatible,'title')\n",
    "    #display(df_pap_scholar_compatible.head(1))\n",
    "\n",
    "    df_biomedRxiv_compatible = df_biomedRxiv[list(dic_biomedRxiv2cord10.keys())]\n",
    "    df_biomedRxiv_compatible.columns = list(dic_biomedRxiv2cord10.values())\n",
    "    hash(df_biomedRxiv_compatible,df_biomedRxiv_compatible,'title')\n",
    "    #display(df_biomedRxiv_compatible.head(1))\n",
    "\n",
    "    df_cord19_meta_augmented = df_cord19_meta.append(df_pap_scholar_compatible, ignore_index=True).append(df_biomedRxiv_compatible, ignore_index=True)\n",
    "    df_cord19_meta_augmented['publish_time'] = df_cord19_meta_augmented['publish_time'].astype('datetime64')\n",
    "    \n",
    "    df_cord19_meta_augmented.to_csv(literature_path+'metadata.csv')\n",
    "\n",
    "    #return df_cord19_meta_augmented\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_df(df):\n",
    "    u = df.abstract.str.split('.',expand=True).stack()\n",
    "\n",
    "    sentences = pd.DataFrame({\n",
    "        'ncord_uid': u.index.get_level_values(0) , \n",
    "        'sentence': u.values\n",
    "    })\n",
    "    \n",
    "    return sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entityset(entityset_name, df):\n",
    "    es = ft.EntitySet(entityset_name)\n",
    "    sentences=create_sentence_df(df)\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id=\"paper\",\n",
    "                                  dataframe=df,\n",
    "                                  index = 'ncord_uid',\n",
    "                                  make_index = True,\n",
    "                                 )\n",
    "\n",
    "    es = es.entity_from_dataframe(entity_id=\"sentence\",\n",
    "                                  dataframe=sentences,\n",
    "                                  index = 'sid',\n",
    "                                  make_index = True,\n",
    "                                 )\n",
    "\n",
    "    es = es.add_relationship(ft.Relationship(es[\"paper\"][\"ncord_uid\"],\n",
    "                                       es[\"sentence\"][\"ncord_uid\"]))\n",
    "\n",
    "    return es\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (13,14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "# Prepare merged `metadata` if not exist\n",
    "download_literature()\n",
    "merge2cord19_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata from data/task1/CORD-19-research-challenge\n",
      "Cleaning metadata\n",
      "Applying tags to metadata\n",
      "\n",
      "Indexing research papers\n",
      "Creating the BM25 index from the abstracts of the papers\n",
      "Use index=\"text\" if you want to index the texts of the paper instead\n",
      "Finished Indexing in 234.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# integrate with CORD19 package for screening/searching\n",
    "research_papers = ResearchPapers.load(data_dir=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching term =  anticoagulants\n",
      "Searching term =  venous thromboembolism\n",
      "Searching term =  thrombotic complications\n",
      "Searching term =  hypercoagulability\n",
      "Searching term =  clot formation\n",
      "Searching term =  Thrombosis\n",
      "Searching term =  Thrombotic\n",
      "Searching term =  D-Dimer\n"
     ]
    }
   ],
   "source": [
    "# Search papers related to keywords\n",
    "fields = ['cord_uid','sha','title','journal','authors','abstract','covid_related','virus','coronavirus','sars','published','when']\n",
    "hypercoagulable =pd.DataFrame(columns = fields)\n",
    "\n",
    "for item in keywords_of_interest:\n",
    "    print('Searching term = ', item)\n",
    "    temp  = research_papers.contains('treat',column='abstract').search(item,num_results=1000)\n",
    "    hypercoagulable = hypercoagulable.append(temp.results[fields], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypercoagulable.reset_index().rename(columns={'index':'ncord_uid'}).to_csv(data_path+'metadata_hypercoagulable.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate sentence level dataframe from all papers\n",
    "# es=create_entityset(\"covid19_complete\", research_papers.metadata[fields])\n",
    "# fulldf = es['sentence'].df.merge(es['paper'].df[['ncord_uid','cord_uid','sha']])\n",
    "# fulldf.rename(columns={'sha':'paper_id'},inplace=True)\n",
    "# fulldf.to_csv(literature_path+'metadata_sentences_with_cord_uid.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sentence level dataframe from hypercoagulabel papers\n",
    "#es_hyper=create_entityset(\"covid19\", hypercoagulable[fields])\n",
    "#es_hyper['sentence'].df.to_csv(data_path+sentence_file_name, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
