{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForQuestionAnswering\n",
    "#from question_answering import load_pretrained_qa_model\n",
    "#from question_answering import answer_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path information\n",
    "task='task1'# or 'task2' # specify task\n",
    "#root_path='/repo1/code/autoreview/'\n",
    "root_path='./'\n",
    "data_path=root_path+'data/'+task+'/'\n",
    "save_path=root_path+'results/'+task+'/'\n",
    "ranking_file_name='round_2_ab.pkl' #'ranking.tsv'\n",
    "question_file_name='questions_structured.csv'\n",
    "#literature_path=data_path+'CORD-19-research-challenge/'#path to save retrieved articles abstract\n",
    "#sentence_file_name='hypercoagulable_sentences.tsv'\n",
    "\n",
    "answer_confidence_threshold=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_qa_model(model_str=None, use_cuda=True):\n",
    "    if model_str is None:\n",
    "        model_str = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() and use_cuda else 'cpu')\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(model_str)\n",
    "    model = BertForQuestionAnswering.from_pretrained(model_str).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "def answer_question(question, document, model, tokenizer):\n",
    "    device = model.device\n",
    "    \n",
    "    encoded = tokenizer.encode_plus(question, document, return_tensors='pt', max_length=512)\n",
    "    start_scores, end_scores = model(encoded['input_ids'].to(device),\n",
    "                                     token_type_ids=encoded['token_type_ids'].to(device))\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'].squeeze())\n",
    "    ans_start, ans_end = torch.argmax(start_scores), torch.argmax(end_scores)\n",
    "    \n",
    "    ans_tokens = tokens[ans_start: ans_end+1]\n",
    "    if '[SEP]' in ans_tokens:\n",
    "        ans_tokens = ans_tokens[ans_tokens.index('[SEP]')+1:]\n",
    "    ans = tokenizer.convert_tokens_to_string(ans_tokens)\n",
    "    ans = ans.replace(' - ', '-').replace('[CLS]', '')\n",
    "    ans_score = start_scores.max() + end_scores.max()\n",
    "\n",
    "    return ans, ans_score.item()\n",
    "\n",
    "def ask_all_questions(abstract, ncord_uid):\n",
    "    answers = []\n",
    "    for question in questions['question'].values:\n",
    "        ans, score= answer_question(question, abstract, model=model, tokenizer=tokenizer)\n",
    "        if ans !='':\n",
    "            answers.append((ncord_uid, question, ans, score))\n",
    "    if len(answers) == 0:\n",
    "        return None\n",
    "    #sids, questions_zip, answers, scores = zip(*answers)\n",
    "    #ans_idx = np.argmax(scores)\n",
    "    #return sids[ans_idx], questions_zip[ans_idx], answers[ans_idx], scores[ans_idx]\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_pretrained_qa_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking=pickle.load(open(save_path+ranking_file_name, 'rb'))\n",
    "#ranking=ranking.drop(columns=['Unnamed: 0'],axis=1).reset_index(drop=True)\n",
    "ranking=ranking.reset_index(drop=True)\n",
    "ranking=ranking.loc[ranking['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions=pd.read_csv(data_path+question_file_name, header=None, names=['type','question'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>study type</td>\n",
       "      <td>Was it a Meta-Analysis study?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>study type</td>\n",
       "      <td>Was it a Systematic Review study?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>study type</td>\n",
       "      <td>Was it a Randomized Controlled Trial?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>study type</td>\n",
       "      <td>Was it a Cohort Study?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>study type</td>\n",
       "      <td>Was it a Case-control Study?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         type                               question\n",
       "0  study type          Was it a Meta-Analysis study?\n",
       "1  study type      Was it a Systematic Review study?\n",
       "2  study type  Was it a Randomized Controlled Trial?\n",
       "3  study type                 Was it a Cohort Study?\n",
       "4  study type           Was it a Case-control Study?"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#answers_all_=ask_all_questions(ranking.loc[0,'abstract'], 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_answers_all=ranking.apply(lambda row: ask_all_questions(row['sentence'], row['ncord_uid']),axis=1)\n",
    "answers_all = [item for list in _answers_all.values for item in list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_all=pd.DataFrame(answers_all, columns=['ncord_uid', 'question', 'answer', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_all=answers_all.loc[answers_all['score']>answer_confidence_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_all_pivot=answers_all.pivot(index='ncord_uid',columns='question', values='answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_all_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_all_pivot.to_csv(save_path+'summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
