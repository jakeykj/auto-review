{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForQuestionAnswering\n",
    "#from question_answering import load_pretrained_qa_model\n",
    "#from question_answering import answer_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path information\n",
    "task='task1'# or 'task2' # specify task\n",
    "#root_path='/repo1/code/autoreview/'\n",
    "root_path='./'\n",
    "data_path=root_path+'data/'+task+'/'\n",
    "save_path=root_path+'results/'+task+'/'\n",
    "submission_path='./'\n",
    "ranking_file_name='pseudo_label.pkl' #'ranking.tsv'\n",
    "question_file_name='questions_structured.csv'\n",
    "literature_file_name='metadata_hypercoagulable.tsv'\n",
    "answer_confidence_threshold=1\n",
    "top_k=300 # top_k articles\n",
    "col='abstract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_qa_model(model_str=None, use_cuda=True):\n",
    "    if model_str is None:\n",
    "        model_str = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() and use_cuda else 'cpu')\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(model_str)\n",
    "    model = BertForQuestionAnswering.from_pretrained(model_str).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "def answer_question(question, document, model, tokenizer):\n",
    "    device = model.device\n",
    "    \n",
    "    encoded = tokenizer.encode_plus(question, document, return_tensors='pt', max_length=512)\n",
    "    start_scores, end_scores = model(encoded['input_ids'].to(device),\n",
    "                                     token_type_ids=encoded['token_type_ids'].to(device))\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'].squeeze())\n",
    "    ans_start, ans_end = torch.argmax(start_scores), torch.argmax(end_scores)\n",
    "    \n",
    "    ans_tokens = tokens[ans_start: ans_end+1]\n",
    "    if '[SEP]' in ans_tokens:\n",
    "        ans_tokens = ans_tokens[ans_tokens.index('[SEP]')+1:]\n",
    "    ans = tokenizer.convert_tokens_to_string(ans_tokens)\n",
    "    ans = ans.replace(' - ', '-').replace('[CLS]', '')\n",
    "    ans_score = start_scores.max() + end_scores.max()\n",
    "\n",
    "    return ans, ans_score.item()\n",
    "\n",
    "def ask_all_questions(abstract, ncord_uid):\n",
    "    answers = []\n",
    "    for question in questions['question'].values:\n",
    "        ans, score= answer_question(question, abstract, model=model, tokenizer=tokenizer)\n",
    "        if ans !='':\n",
    "            answers.append((ncord_uid, question, ans, score))\n",
    "    if len(answers) == 0:\n",
    "        return None\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load QA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_pretrained_qa_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare ranked articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking=pickle.load(open(save_path+ranking_file_name, 'rb'))\n",
    "#ranking=ranking.drop(columns=['Unnamed: 0'],axis=1).reset_index(drop=True)\n",
    "ranking=ranking.reset_index(drop=True)\n",
    "ranking=ranking.loc[ranking['label']==1]\n",
    "ranking_top_k=ranking.iloc[:top_k,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask questions and get answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions=pd.read_csv(data_path+question_file_name, header=None, names=['type','question'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_answers_all=ranking_top_k.apply(lambda row: ask_all_questions(row[col], row['ncord_uid']),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_all = [item for list in _answers_all.dropna().values for item in list]\n",
    "answers_all=pd.DataFrame(answers_all, columns=['ncord_uid', 'question', 'answer', 'score'])\n",
    "answers_all=answers_all.loc[answers_all['score']>answer_confidence_threshold]\n",
    "answers_all_pivot=answers_all.pivot(index='ncord_uid',columns='question', values='answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: set_axis currently defaults to operating inplace.\n",
      "This will change in a future version of pandas, use inplace=True to avoid this warning.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "index_category_question=pd.MultiIndex.from_frame(questions, names=['category','question'])\n",
    "answers_all_pivot.set_axis(index_category_question, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load article's meta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "literature=pd.read_csv(data_path+literature_file_name, sep='\\t', index_col='ncord_uid').drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: set_axis currently defaults to operating inplace.\n",
      "This will change in a future version of pandas, use inplace=True to avoid this warning.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "metadata=['published','journal', 'title', 'abstract','cord_uid']\n",
    "idx=[('meta', col) for col in metadata]\n",
    "idx=pd.MultiIndex.from_tuples(idx)\n",
    "literature_multiindex=literature[metadata]\n",
    "literature_multiindex.set_axis(idx, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the meta information with extracted information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary=pd.merge(answers_all_pivot, literature_multiindex, how='left',left_index=True, right_index=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv(submission_path+'summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
