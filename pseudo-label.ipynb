{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "import os\n",
    "from snorkel.labeling import PandasLFApplier,LFAnalysis,LabelingFunction\n",
    "from snorkel.labeling.model.label_model import LabelModel\n",
    "from snorkel.labeling.apply.dask import PandasParallelLFApplier\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import pytrends\n",
    "from pytrends.request import TrendReq\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime, date, time\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path information\n",
    "task='task1'# or 'task2' # specify task\n",
    "root_path='/repo1/code/autoreview/'\n",
    "data_path=root_path+'data/'+task+'/'#path to save retrieved articles abstract\n",
    "keyword_path=data_path+'keywords/'#keyword list\n",
    "save_path=root_path+'results/'+task+'/'\n",
    "sentence_file_name='hypercoagulable.pkl'\n",
    "sentence_embedding_file_name='sentence_embedding.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keyword(keyword_file_name):\n",
    "    \"\"\"\n",
    "    Read keyword list\n",
    "    \n",
    "    input:\n",
    "        keyword_type_name: str, file name, e.g., `keylist.txt`\n",
    "    output:\n",
    "        list of string, e.g., ['keyword1', 'keyword2', 'keyword3']\n",
    "        \n",
    "    \"\"\"\n",
    "    with open(keyword_file_name, \"r\") as f:\n",
    "        keylist=f.read().split(',')\n",
    "    return keylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_labing(keywordslist,viruslist,triallist,mustlist,maxngramnu):\n",
    "    \n",
    "    # keywordslist is the concept we want\n",
    "    # viruslist is what type of virus we want to loop up\n",
    "    # triallist is concept of trial we want to loop up\n",
    "    # mustlist is concept of  we want the high ranking sentence should have\n",
    "    # maxngramnu is the maxinum number of word window\n",
    "    \n",
    "    # lookup keywords in n-words window (for controling the window, we set up the max number of word window)\n",
    "    def keyword_lookup(x,keywords,maxngram,label):\n",
    "        threshold=0\n",
    "        for i in range(maxngram):\n",
    "            gramab=ngrams(x.sentence.lower().split(), i+1)\n",
    "            for word in gramab:\n",
    "                wordlist=list(word)\n",
    "                if any(word1 in wordlist  for word1 in keywords):\n",
    "                    threshold=threshold+1\n",
    "        if threshold>0:\n",
    "            return label\n",
    "        return Norelevent\n",
    "\n",
    "    def make_keyword_lf(keywords,maxngram,name,label=None):\n",
    "        return LabelingFunction(\n",
    "            name=f\"keyword_{name}\",\n",
    "            f=keyword_lookup,\n",
    "            resources=dict(keywords=keywords,maxngram=maxngram,label=label),)\n",
    "    \n",
    "    # lookup both group keywords in n-words window (for controling the window, we set up the max number of word window)\n",
    "    \n",
    "    def keyword_lookup_b(x,keywords1,keywords2,maxngram,label):\n",
    "        threshold=0\n",
    "        for i in range(maxngram):\n",
    "            gramab=ngrams(x.sentence.lower().split(), i+1)\n",
    "            for word in gramab:\n",
    "                wordlist=list(word)\n",
    "                if any(word1 in wordlist  for word1 in keywords1) and any(word1 in wordlist  for word1 in keywords2,):\n",
    "                    threshold=threshold+1\n",
    "        if threshold>0:\n",
    "            return label\n",
    "        return Norelevent\n",
    "\n",
    "    def make_keyword_b_lf(keywords1,keywords2,maxngram,name,label=None):\n",
    "        return LabelingFunction(\n",
    "            name=f\"keyword_b_{name}\",\n",
    "            f=keyword_lookup_b,\n",
    "            resources=dict(keywords1=keywords1,keywords2=keywords2,maxngram=maxngram,label=label),)\n",
    "   \n",
    "    # lookup a group keywords presenting and not presenting in another group of keywords\n",
    "    \n",
    "    def not_lookup_b(x,keywords1,keywords2,maxngram,label):\n",
    "        threshold=0\n",
    "        for i in range(maxngram):\n",
    "            gramab=ngrams(x.sentence.lower().split(), i+1)\n",
    "            for word in gramab:\n",
    "                wordlist=list(word)\n",
    "                if any(word1 in wordlist  for word1 in keywords2):\n",
    "                    threshold=threshold+1\n",
    "        threshold1=0\n",
    "        for i in range(maxngram):\n",
    "            gramab=ngrams(x.sentence.lower().split(), i+1)\n",
    "            for word in gramab:\n",
    "                wordlist=list(word)\n",
    "                if any(word1 in wordlist  for word1 in keywords1):\n",
    "                    threshold1=threshold1+1\n",
    "        if threshold<1 and threshold1>0:\n",
    "            return label\n",
    "        return Norelevent\n",
    "\n",
    "    \n",
    "    def make_not_b_lf(keywords1,keywords2,name,maxngram,label=None):\n",
    "        return LabelingFunction(\n",
    "            name=f\"not_b_{name}\",\n",
    "            f=not_lookup_b,\n",
    "            resources=dict(keywords1=keywords1,keywords2=keywords2,maxngram=maxngram,label=label),)   \n",
    "    \n",
    "    \n",
    "      # lookup up number in sentence \n",
    "    \n",
    "    \n",
    "    def number_lookup(x,keywords,label):\n",
    "        threshold=0\n",
    "        sentlist=x.sentence.split()\n",
    "        trialindex=[i for i, e in enumerate(sentlist) if e in keywords]\n",
    "        nuindex=[i for i, e in enumerate(sentlist) if str(e).isdigit()]\n",
    "        for i in trialindex:\n",
    "            for j in nuindex:\n",
    "                if i<j:\n",
    "                    threshold+=1\n",
    "        if threshold>0:\n",
    "            return label\n",
    "        return Norelevent\n",
    "\n",
    "    def make_number_lf(keywords,label=None):\n",
    "        return LabelingFunction(\n",
    "            name=f\"number\",\n",
    "            f=number_lookup,\n",
    "            resources=dict(keywords=keywords,label=label),)\n",
    "\n",
    "   \n",
    "    \n",
    "    Norelevent = -1\n",
    "    \n",
    "    keywordfu=make_keyword_lf(keywords=keywordslist,maxngram=maxngramnu,name='keyword',label=1)    \n",
    "    virusfu=make_keyword_lf(keywords=viruslist,maxngram=maxngramnu,name='virus',label=1)\n",
    "    keywordfub=make_keyword_b_lf(keywords1=keywordslist,keywords2=mustlist,maxngram=maxngramnu,name='keyword',label=1)    \n",
    "    virusfub=make_keyword_b_lf(keywords1=viruslist,keywords2=mustlist,maxngram=maxngramnu,name='virus',label=1)\n",
    "    trialfu=make_keyword_lf(keywords=triallist,name='trial',maxngram=maxngramnu,label=1)\n",
    "    notfu1=make_not_b_lf(keywords1=mustlist,keywords2=keywordslist,maxngram=maxngramnu,name='notkeyword',label=0)\n",
    "    notfu2=make_not_b_lf(keywords1=keywordslist,keywords2=triallist,maxngram=maxngramnu,name='nottrial',label=0)\n",
    "    numberfu=make_number_lf(keywords=triallist,label=1)\n",
    "    \n",
    "    allweaklabf=[]\n",
    "\n",
    "    allweaklabf.append(keywordfu)\n",
    "    allweaklabf.append(virusfu)\n",
    "    allweaklabf.append(keywordfub)\n",
    "    allweaklabf.append(virusfub)\n",
    "    allweaklabf.append(trialfu)\n",
    "    allweaklabf.append(notfu1)\n",
    "    allweaklabf.append(notfu2)\n",
    "    allweaklabf.append(numberfu)\n",
    "\n",
    "    \n",
    "    return allweaklabf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_prob(sentences,lfs,cardinalitynu):\n",
    "    \"\"\"\n",
    "    Predict probability (label) by applying label functions lfs\n",
    "    Refined from `snorkel_process` \n",
    "    \n",
    "    input:\n",
    "        lfs: list of snorkel.LabelingFunction\n",
    "    output:\n",
    "        sentences_labeled: pd.Dataframe,  [pid, sid, sent, label, prob ]\n",
    "    \"\"\"\n",
    "    \n",
    "    applier=PandasLFApplier(lfs=lfs)\n",
    "    applied=applier.apply(df=sentences)\n",
    "    print(LFAnalysis(L=applied, lfs=lfs).lf_summary())\n",
    "    \n",
    "    label_model = LabelModel(cardinality=cardinalitynu,verbose=True)\n",
    "    label_model.fit(applied)\n",
    "    sentences['label']=label_model.predict(applied)\n",
    "    sentences['prob']=label_model.predict_proba(applied)[:,1]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8766/8766 [00:12<00:00, 686.55it/s]\n"
     ]
    }
   ],
   "source": [
    "#abstract\n",
    "gold=pd.read_csv('hypercoagulable_sentences.csv')\n",
    "gold=gold.dropna()\n",
    "ab=[]\n",
    "ncord=[]\n",
    "for i in tqdm(gold.ncord_uid.drop_duplicates()):\n",
    "    temp=gold[gold.ncord_uid==i]\n",
    "    tempsent=temp.sentence.values\n",
    "    tempncord=temp.ncord_uid.drop_duplicates().values[0]\n",
    "    init=''\n",
    "    for j in tempsent:\n",
    "        init=init+j\n",
    "    ab.append(init)\n",
    "    ncord.append(tempncord)\n",
    "goldab=pd.DataFrame(ncord,columns=['ncord_uid'])\n",
    "goldab['sentence']=ab\n",
    "goldab=goldab.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8766/8766 [02:43<00:00, 53.70it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   j Polarity  Coverage  Overlaps  Conflicts\n",
      "keyword_keyword    0      [1]  0.247205  0.247205   0.133356\n",
      "keyword_virus      1      [1]  0.060575  0.050650   0.006160\n",
      "keyword_b_keyword  2       []  0.000000  0.000000   0.000000\n",
      "keyword_b_virus    3      [1]  0.000570  0.000570   0.000570\n",
      "keyword_trial      4      [1]  0.628109  0.423340   0.000684\n",
      "not_b_notkeyword   5      [0]  0.001141  0.000799   0.000799\n",
      "not_b_nottrial     6      [0]  0.133356  0.133356   0.133356\n",
      "number             7      [1]  0.354438  0.354438   0.000228\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cardinalitynu=2\n",
    "allweaklabf=loop_labing(keywordslist,viruslist,triallist,mustlist,3)\n",
    "sentence=predict_prob(goldab,allweaklabf,cardinalitynu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=sentence.sort_values(by='prob',ascending=False)\n",
    "sentence.to_pickle('round_2_ab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 142901/142901 [04:09<00:00, 572.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   j Polarity  Coverage  Overlaps  Conflicts\n",
      "keyword_keyword    0      [1]  0.018810  0.018810   0.015997\n",
      "keyword_virus      1      [1]  0.015136  0.003261   0.001274\n",
      "keyword_b_keyword  2       []  0.000000  0.000000   0.000000\n",
      "keyword_b_virus    3      [1]  0.000035  0.000035   0.000035\n",
      "keyword_trial      4      [1]  0.078222  0.013121   0.000021\n",
      "not_b_notkeyword   5      [0]  0.000203  0.000140   0.000140\n",
      "not_b_nottrial     6      [0]  0.015997  0.015997   0.015997\n",
      "number             7      [1]  0.009251  0.009251   0.000000\n"
     ]
    }
   ],
   "source": [
    "gold=pd.read_csv('hypercoagulable_sentences.tsv',sep='\\t')\n",
    "gold['sentence']=gold.sentence.astype(str)\n",
    "cardinalitynu=2\n",
    "allweaklabf=loop_labing(keywordslist,viruslist,triallist,mustlist,3)\n",
    "sentence=predict_prob(gold,allweaklabf,cardinalitynu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=sentence.sort_values(by='prob',ascending=False)\n",
    "sentence.to_pickle('round_2_sent.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardinalitynu=2\n",
    "allweaklabf=loop_labing(keywordslist,viruslist,triallist,mustlist,3)\n",
    "applier = PandasLFApplier(lfs=allweaklabf)\n",
    "all_train_l = applier.apply(df=gold)\n",
    "#applier = PandasParallelLFApplier(lfs=allweaklabf)\n",
    "#all_train_l = applier.apply(df=allframe1,n_parallel=40)\n",
    "report=LFAnalysis(L=all_train_l, lfs=allweaklabf).lf_summary()\n",
    "print(report)\n",
    "label_model = LabelModel(cardinality=cardinalitynu,verbose=True)\n",
    "label_model.fit(all_train_l)\n",
    "predt=label_model.predict(all_train_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold['label']=predt\n",
    "prob=label_model.predict_proba(all_train_l)[:,1]\n",
    "gold['prob_snokel']=prob\n",
    "gold1=gold[gold.label==1]\n",
    "gold1=gold1.drop_duplicates()\n",
    "gold1=gold1.sort_values(by='prob_snokel',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold.to_pickle('./round_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
